# Detecting Credit Card Fraud

# Importing Datasets
#install.packages("datasets")
library(ranger)
library(caret)
library(data.table)
#format of import is ("path of csv//csv file name") backslash change after copy and C remove
creditcard_data <- read.csv("/Users/KIIT/Documents/1 My Documents/Project Data Sets//creditcard.csv")

# Data Exploration
dim(creditcard_data) #returns dimension in (col,row) #here shows (284807,37)
head(creditcard_data,6) #first n rows of data set as below output showing V1 to V7 then next line V8 to V14
tail(creditcard_data,6) #last n rows here n=6
#table(categorical variable from class acess)
table(creditcard_data$Class) #total of 284,807 transactions, out of which 492 are fraudulent
#table function gives frequency of any categorical variable in data set
#above dollar operator is used to access, add, delete,...
summary(creditcard_data$Amount)#above summary function summaries data frame wrt Amount 
#summaries with specific parameters of min 1st auad mediean mean 3rd Quad and max
names(creditcard_data) #displays all coloumn names of data frame
var(creditcard_data$Amount)
#variance formula on amount column
#variance gives the measure of the spread or dispersion within a set of data
sd(creditcard_data$Amount) #standard deviation on amount column
#SD is the measure of how dispersed the data is in relation to the mean

# Data Manipulation
head(creditcard_data) #gives first 6 rows of a data default of the data set
creditcard_data$Amount=scale(creditcard_data$Amount)
#This function standardizes the values of "Amount" coloumn by subtracting the mean and dividing by the standard deviation, 
#resulting in a new variable with a mean of 0 and a standard deviation of 1.
NewData=creditcard_data[,-c(1)] #-ve sign indicates removal of 1st c=coloumn from data set
head(NewData) #displays 1st few rows of new data set

# Data Modelling
#install.packages("caTools")
#METHOD to split data into training and testing Using caTools package
library(caTools)
set.seed(123) #sets the random number generator's seed value to 123
#in RNG Algorithm-number generated by RNG seed value set to 123
data_sample = sample.split(NewData$Class,SplitRatio=0.80) #splits new data into train & test based on class variable
#0.80 so use 80% of data set for training and rest 20% for testing  
#sample.split() is a function from the caret package that is used to split a data set into training and testing sets on a user-specified split ratio.
train_data = subset(NewData,data_sample==TRUE) #a subset to train a model
test_data = subset(NewData,data_sample==FALSE) #a subset to test the trained model
dim(train_data) #gives dimension of train_data set
dim(test_data) 
# Fitting Logistic Regression Model-LR is part of a larger class of Generalized Linear Model-->glm function used for fitting generalized linear models
Logistic_Model=glm(Class~.,test_data,family=binomial())
#class~ specifies formula for logistic regression on testing data
summary(Logistic_Model)
# Visualizing summarized model through the following plots
plot(Logistic_Model)
# ROC Curve to assess the performance of the model
library(pROC)
lr.predict <- predict(Logistic_Model,test_data, probability = TRUE)
auc.gbm = roc(test_data$Class, lr.predict, plot = TRUE, col = "blue")
# Fitting a Decision Tree Model
library(rpart)
library(rpart.plot)
decisionTree_model <- rpart(Class ~ . , creditcard_data, method = 'class')
predicted_val <- predict(decisionTree_model, creditcard_data, type = 'class')
probability <- predict(decisionTree_model, creditcard_data, type = 'prob')
rpart.plot(decisionTree_model)
# Artificial Neural Network
library(neuralnet)
ANN_model =neuralnet (Class~.,train_data,linear.output=FALSE)
plot(ANN_model)
predANN=compute(ANN_model,test_data)
resultANN=predANN$net.result
resultANN=ifelse(resultANN>0.5,1,0)
# Gradient Boosting (GBM)
library(gbm, quietly=TRUE)
# Get the time to train the GBM model
system.time(
  model_gbm <- gbm(Class ~ .
                   , distribution = "bernoulli"
                   , data = rbind(train_data, test_data)
                   , n.trees = 500
                   , interaction.depth = 3
                   , n.minobsinnode = 100
                   , shrinkage = 0.01
                   , bag.fraction = 0.5
                   , train.fraction = nrow(train_data) / (nrow(train_data) + nrow(test_data))
  )
)
# Determine best iteration based on test data
gbm.iter = gbm.perf(model_gbm, method = "test")
model.influence = relative.influence(model_gbm, n.trees = gbm.iter, sort. = TRUE)
#Plot the gbm model
plot(model_gbm)
#Plot and calculate AUC on test data
library(pROC)
gbm_test = predict(model_gbm, newdata = test_data, n.trees = gbm.iter)
gbm_auc = roc(test_data$Class, gbm_test, plot = TRUE, col = "red")
print(gbm_auc)
























